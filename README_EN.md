> âš ï¸ This project also has a Chinese version: [README.md](README.md)

## ğŸš€ Project Overview
Based on Hugging Face's `bert-base-chinese` and `chinese-bert-wwm` models, this project performs Chinese Named Entity Recognition (NER) tasks, helping users understand the logic of sequence labeling model construction and evaluation metrics.

### Implemented Modules
- **Data Loading and Preprocessing** (BIO labeling format)  
- **Model Design and Training Pipeline**  
- **Model Evaluation and Result Visualization** (visualization uses `swanlab`)  

## âœ… Experimental Results
Evaluation metrics include **F1**, **Precision**, and **Recall**.  
The following results are on `test.txt`:

| Dataset | Model                | Precision | Recall | F1 Score |
|---------|--------------------|-----------|--------|----------|
| weibo   | bert-base-chinese   | 0.6284    | 0.6634 | 0.6455   |
| weibo   | chinese-bert-wwm    | 0.6117    | 0.6828 | 0.6453   |
| msra    | bert-base-chinese   | 0.9431    | 0.9429 | 0.9430   |
| msra    | chinese-bert-wwm    | 0.9365    | 0.9365 | 0.9365   |

Results on `dev.txt` can be found in `result/training_log.txt`.

## How to Run the Code

### 1. Install Dependencies

```bash
pip install -r requirements.txt

```

### 2.Train the Model
- --mode: run mode, set to train for training

- --config_path: path to the configuration file, optional (default: ./NER_Config/Bertbase_Weibo_Config.json)
Each config file corresponds to a single experiment.

```bash
python main.py --mode train --config_path ./NER_Config/Bertbase_Weibo_Config.json

```
After training, the model weights will be saved in the directory specified by trained_save_root_path in the config file, and the file path will also be recorded in the JSON.

### 3. Evaluate on the Test Set
```bash
python main.py --mode eval --config_path ./NER_Config/Bertbase_Weibo_Config.json

```

The script will output F1, Precision, Recall, and record the results in training_log.txt.

### 4.Predict Entities in a Single Sentence
```bash
python main.py --mode predict --config_path ./NER_Config/Bertbase_Weibo_Config.json

```

The program will prompt you to input a Chinese sentence.
It will output the entities and their types, e.g.:
```bash

[('å°æ˜', 'PER.NAM'), ('åŒ—äº¬', 'GPE.NAM'), ('å¤§å­¦', 'ORG.NOM')]

```
![](https://img.xlonglong.cn/img/202511181811635.png)


## ğŸ“Š Dataset Sources

1. **Weibo NER Dataset**
    - Description: This dataset includes training (1350), validation (269), and test (270) sets. Entity types include geopolitical entities (GPE.NAM), locations (LOC.NAM), organizations (ORG.NAM), and people (PER.NAM) along with nominal forms (NOM).  
    - Language: Chinese  
    - Train/Dev/Test sizes: 1350/269/270  
    - Number of entity categories: 4  
    - Paper: [https://aclanthology.org/D15-1064.pdf](https://aclanthology.org/D15-1064.pdf)  
    - Download: [https://tianchi.aliyun.com/dataset/144312](https://tianchi.aliyun.com/dataset/144312)  
    - GitHub: [https://github.com/hltcoe/golden-horse](https://github.com/hltcoe/golden-horse)

2. **MSRA NER Dataset**
    - Description: The MSRA dataset is a Chinese NER dataset in the news domain, including training (46364) and test (4365) sets. Entity types include locations (LOC), people (NAME), and organizations (ORG).  
    - Language: Chinese  
    - Train/Test sizes: 46364/4365 (The training set was re-split into train and dev sets using `data_split_tools.py`)  
    - Number of entity categories: 3  
    - Paper: [https://aclanthology.org/W06-0115.pdf](https://aclanthology.org/W06-0115.pdf)  
    - Download: [https://tianchi.aliyun.com/dataset/144307](https://tianchi.aliyun.com/dataset/144307)

## ğŸ“‚ Project Structure Example
> Note: final model weights and pretrained model folders are not uploaded

```
Weibo-NER/
â”œâ”€â”€ model/ 
â”‚ â”œâ”€â”€ models--bert-base-chinese
â”‚ â”œâ”€â”€ models--hfl--chinese-bert-wwm
â”œâ”€â”€ data/ 
â”‚ â”œâ”€â”€ msra_NER
â”‚   â”œâ”€â”€ train.txt #åŸå§‹æ•°æ®
â”‚   â”œâ”€â”€ train_split.txt
â”‚   â”œâ”€â”€ dev_split.txt
â”‚   â”œâ”€â”€ test.txt
â”‚   â”œâ”€â”€msra_ner.json
â”‚ â”œâ”€â”€Weibo_NER
â”‚   â”œâ”€â”€ train.txt
â”‚   â”œâ”€â”€ test.txt
â”‚   â”œâ”€â”€ dev.txt
â”‚   â”œâ”€â”€ class.txt
â”œâ”€â”€ result/ #å®éªŒç»“æœ
â”‚ â”œâ”€â”€ bert-base-chinese_for_Weibo_NER.pth
â”‚ â”œâ”€â”€ bert-base-chineseforWeibo_NER.pth
â”‚ â”œâ”€â”€ chinese-bert-wwmformsra_NER.pth
â”‚ â”œâ”€â”€ chinese-bert-wwm_for_Weibo_NER.pth
â”‚ â”œâ”€â”€ training_log.txt
â”œâ”€â”€ NER_config/
â”‚ â”œâ”€â”€ Bertbase_msra_Config.json
â”‚ â”œâ”€â”€ Bertbase_Weibo_Config.json
â”‚ â”œâ”€â”€ Chinese_bert_wwm_msra_config.json.json
â”‚ â”œâ”€â”€ Chinese_bert_wwm_Weibo_config.json.json
â”‚ 
â”‚â”€â”€ data_process.py
â”‚â”€â”€ main.py
â”‚â”€â”€ model.py
â”‚â”€â”€ train_evaluate.py
â”œâ”€â”€ requirements.txt
â”‚â”€â”€ My_Config.py
â”œâ”€â”€ data_split_tools.py
â”‚â”€â”€ downloadmodel.py
â”‚â”€â”€ tools.py
â””â”€â”€ README.md
â””â”€â”€ README_EN.md

```
